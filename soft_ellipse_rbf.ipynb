{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Ellipse RBF for Non-Convex Shape Classification\n",
    "\n",
    "**A Neural Network Approach Using Soft Elliptical Radial Basis Functions**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Abstract](#1-abstract)\n",
    "2. [Environment Setup & Configuration](#2-environment-setup--configuration)\n",
    "3. [Core Implementation](#3-core-implementation)\n",
    "   - 3.1 [Utilities](#31-utilities--random-seed-management)\n",
    "   - 3.2 [Data Generation Functions](#32-data-generation-functions)\n",
    "   - 3.3 [Soft Ellipse RBF Layer](#33-soft-ellipse-rbf-layer)\n",
    "   - 3.4 [Incremental Model Architecture](#34-incremental-model-architecture)\n",
    "4. [Training Strategy](#4-training-strategy)\n",
    "   - 4.1 [Pruning and Clustering](#41-pruning-and-dbscan-based-clustering)\n",
    "   - 4.2 [Base Stage: Initial Unit Placement](#42-base-stage-initial-unit-placement)\n",
    "   - 4.3 [ECR Stage: Error Cluster Refinement](#43-ecr-stage-error-cluster-refinement)\n",
    "5. [Training Pipeline & Visualization](#5-training-pipeline--visualization)\n",
    "6. [Experimental Results](#6-experimental-results)\n",
    "7. [Interactive Drawing Canvas](#7-interactive-drawing-canvas)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Abstract\n",
    "\n",
    "This notebook demonstrates a **soft ellipse radial basis function (RBF)** network that performs non‑convex shape classification.  We incrementally build the RBF model by adding elliptical units to cover the positive region, refine on error clusters using DBSCAN, and prune away insignificant units. \n",
    "The training pipeline proceeds in three stages:\n",
    "1. **Base Stage**: Use DBSCAN on false negatives to guide the placement of initial units.\n",
    "2. **Error Cluster Refinement (ECR) Stage**: Add extra units on false positives/negatives using density‑based clustering.\n",
    "3. **Final Stage**: Fine‑tune the model with all units active.\n",
    "\n",
    "Additionally, we visualize the evolving decision boundary after each epoch (*plot_frame_interval=5*) and log unit operations in detail (*verbose=1*).\n",
    "\n",
    "---\n",
    "\n",
    "### Example Results\n",
    "\n",
    "![L-shape classification](assets/output_L_shape.png)  \n",
    "*Decision boundary after training on L-shape dataset*\n",
    "\n",
    "![S-shape classification](assets/output_S_shape.png)  \n",
    "*Model handling a non-convex S-shape*\n",
    "\n",
    "![Donut classification](assets/output_donut_shape.png)  \n",
    "*Donut dataset with elliptical units covering the ring*\n",
    "\n",
    "![Custom canvas](assets/input_proc_custom_smiley.png)  \n",
    "*Custom canvas dataset*\n",
    "\n",
    "![Custom canvas classification](assets/output_custom_smiley.png)  \n",
    "*Custom canvas classification result*\n",
    "\n",
    "### Key Implementation Features:\n",
    "\n",
    "**🔧 Layer Parameters:**\n",
    "- **Centers**: 2D coordinates (x, y) defining ellipse center positions\n",
    "- **Semi-axes**: (a, b) controlling ellipse width and height  \n",
    "- **Rotation angle**: ∠°(a-axis vs. Ox) for ellipse orientation  \n",
    "- **Sigmoid sharpness**: K=10 for soft boundary transitions\n",
    "\n",
    "**📊 Training Process:**\n",
    "1. **Base Stage**: Incrementally adds RBF units targeting false negative clusters using DBSCAN\n",
    "2. **ECR (Error Cluster Refinement)**: Refines model by addressing both false positives and false negatives\n",
    "3. **Final Stage**: Fine-tuning with optimized hyperparameters\n",
    "\n",
    "**🎯 Key Algorithms:**\n",
    "- **DBSCAN clustering** for identifying error regions requiring new RBF units\n",
    "- **Fast medoid selection** for optimal RBF center placement\n",
    "- **Volume-based pruning** removes insignificant units (threshold: 1e-3)\n",
    "\n",
    "The method achieves high IoU scores (>95%) on complex non-convex shapes including stars, donuts, and custom polygons while maintaining interpretable elliptical decision boundaries.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup & Configuration\n",
    "\n",
    "Setting up the environment with optimized TensorFlow configuration and importing required libraries for the soft ellipse RBF implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorFlow warnings and info messages - MUST BE FIRST!\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Set environment variables BEFORE importing TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress all TensorFlow logging\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN optimizations messages\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Disable CUDA messages (force CPU)\n",
    "\n",
    "# Additional warning suppression\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "from typing import Callable, Optional\n",
    "from time import sleep\n",
    "import random, numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN, OPTICS, KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import display, update_display, clear_output\n",
    "from ipywidgets import Output, VBox\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')  # Only show TensorFlow errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters & Configuration\n",
    "\n",
    "Comprehensive parameter settings for the three-stage training process, optimized for non-convex shape learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING CONSTANTS\n",
    "TEST_SIZE = 0.3\n",
    "K_SIGMOID = 10  # Sigmoid sharpness for soft ellipse boundaries\n",
    "PRUNE_BUMP_VOLUME_THRESHOLD = 1e-3  # Threshold for pruning small RBF units\n",
    "\n",
    "# Base Stage - Initial RBF unit placement\n",
    "BASE_MAX_UNITS = 7      # Maximum number of RBF units for Base Stage\n",
    "BASE_BATCH = 16         # Batch size for base stage\n",
    "BASE_EPOCHS = 50        # Number of base stage epochs \n",
    "BASE_ADAM = 3e-4        # Learning rate for Adam optimizer \n",
    "BASE_IOU_TARGET = 0.90  # IoU target for early stopping\n",
    "BASE_PATIENCE = 15      # Patience for early stopping\n",
    "BASE_MIN_DELTA = 2e-5   # Minimum delta for early stopping\n",
    "\n",
    "# ECR Stage - Error Cluster Refinement by Biting & Patching\n",
    "ECR_MAX_UNITS = 5       # How many extra units for ECR\n",
    "ECR_BATCH = 24          # Batch size for ECR stage\n",
    "ECR_EPOCHS = 50         # Number of ECR epochs\n",
    "ECR_ADAM = 5e-4         # Learning rate for Adam optimizer\n",
    "ECR_IOU_TARGET = 0.95   # ECR IoU target\n",
    "ECR_PATIENCE = 20       # Patience ECR early stopping\n",
    "ECR_MIN_DELTA = 2e-6    # Minimum delta ECR early stopping\n",
    "\n",
    "# Final Stage - Fine-tuning\n",
    "FINAL_BATCH = 32\n",
    "FINAL_EPOCHS = 75\n",
    "FINAL_ADAM = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Core Implementation\n",
    "\n",
    "### 3.1 Utilities\n",
    "\n",
    "Essential utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_op(*args, **kwargs):\n",
    "    \"\"\"A do-nothing placeholder function for optional callbacks/logging.\"\"\"\n",
    "    pass\n",
    "\n",
    "class RndSeed:\n",
    "    \"\"\"Centralized random seed management for reproducible experiments.\"\"\"\n",
    "    _seed: Optional[int] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def set_seed(cls, seed: int) -> None:\n",
    "        \"\"\"Set random seed across all ecosystems for reproducibility.\"\"\"\n",
    "        cls._seed = seed\n",
    "\n",
    "        # Apply across ecosystems\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)  # affects hashing-based ops\n",
    "        random.seed(seed)                         # Python built-in RNG\n",
    "        np.random.seed(seed)                      # NumPy RNG\n",
    "        tf.keras.utils.set_random_seed(seed)      # TF + NumPy + Python        \n",
    "\n",
    "        print(f\"🎲 Random seed set to: {seed}\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_seed(cls) -> int:\n",
    "        \"\"\"Get current seed, initializing if needed.\"\"\"\n",
    "        if cls._seed is None:\n",
    "            # lazy init with silent auto-seed\n",
    "            cls.set_seed(np.random.randint(0, 2**31 - 1))\n",
    "        return cls._seed # type: ignore\n",
    "\n",
    "def fast_medoid(points, n_sample=500):\n",
    "    \"\"\"Compute approximate medoid of points using sampling for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        points: Array of 2D points\n",
    "        n_sample: Maximum points to sample for medoid computation\n",
    "        \n",
    "    Returns:\n",
    "        Medoid point coordinates\n",
    "    \"\"\"\n",
    "    N = len(points)\n",
    "    if N == 0:\n",
    "        return None\n",
    "    if N > n_sample:\n",
    "        idx_sample = np.random.choice(N, n_sample, replace=False)\n",
    "        sample = points[idx_sample]\n",
    "    else:\n",
    "        sample = points\n",
    "    \n",
    "    dists = np.linalg.norm(sample[:, None, :] - sample[None, :, :], axis=2)\n",
    "    medoid_idx = np.argmin(dists.sum(axis=1))\n",
    "    medoid_point = sample[medoid_idx]\n",
    "    return medoid_point\n",
    "\n",
    "def compute_iou(model, X, y):\n",
    "    \"\"\"Compute Intersection over Union (IoU) metric for binary classification.\"\"\"\n",
    "    preds = (model.predict(X, verbose=0).reshape(-1) > 0.5).astype(int)\n",
    "    inter = np.sum((preds==1) & (y==1))\n",
    "    union = np.sum((preds==1) | (y==1))\n",
    "    return inter/union if union>0 else 0.0\n",
    "\n",
    "def plot_decision_boundary(model, X_data, y_data, ax, grid_resolution=100, title=\"Decision Boundary\"):\n",
    "    \"\"\"Unified function to plot decision boundary with data points.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model with predict method\n",
    "        X_data: Data points to scatter plot\n",
    "        y_data: True labels for the data points\n",
    "        ax: Matplotlib axis to plot on (required)\n",
    "        grid_resolution: Resolution of the decision boundary grid\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Create grid for decision boundary\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(0, 1, grid_resolution),\n",
    "        np.linspace(0, 1, grid_resolution)\n",
    "    )\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # Get model predictions on grid\n",
    "    probs = model.predict(grid, verbose=0).reshape(xx.shape)\n",
    "    \n",
    "    # Plot on provided axis\n",
    "    ax.contourf(xx, yy, probs, levels=50, cmap=\"RdBu\", alpha=0.6)\n",
    "    ax.contour(xx, yy, probs, levels=[0.5], colors=\"black\", linewidths=2)\n",
    "    ax.scatter(X_data[:,0], X_data[:,1], c=y_data, cmap=\"bwr\", edgecolor=\"k\", s=12, alpha=0.6)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Generation Functions\n",
    "\n",
    "Collection of synthetic data generators for testing the RBF network on various non-convex shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polygon_data(n=2000, corners=None):\n",
    "    \"\"\"Generate binary classification data for arbitrary polygon shapes.\n",
    "    \n",
    "    Args:\n",
    "        n: Number of data points to generate\n",
    "        corners: List of (x, y) tuples defining polygon vertices\n",
    "        \n",
    "    Returns:\n",
    "        X: Data points (n, 2)\n",
    "        y: Binary labels (inside=1, outside=0)\n",
    "        corners: Polygon corner coordinates\n",
    "    \"\"\"\n",
    "    if corners is None:\n",
    "        corners = [(0.5, 0.8), (0.1, 0.1), (0.9, 0)]\n",
    "\n",
    "    X = np.random.uniform(0, 1, size=(n, 2))\n",
    "\n",
    "    def point_in_polygon(point, polygon_corners):\n",
    "        \"\"\"Ray casting algorithm for point-in-polygon test.\"\"\"\n",
    "        x, y = point\n",
    "        n = len(polygon_corners)\n",
    "        inside = False\n",
    "        p1x, p1y = polygon_corners[0]\n",
    "        for i in range(1, n + 1):\n",
    "            p2x, p2y = polygon_corners[i % n]\n",
    "            if y > min(p1y, p2y):\n",
    "                if y <= max(p1y, p2y):\n",
    "                    if x <= max(p1x, p2x):\n",
    "                        if p1y != p2y:\n",
    "                            xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n",
    "                        if p1x == p2x or x <= xinters:\n",
    "                            inside = not inside\n",
    "            p1x, p1y = p2x, p2y\n",
    "        return inside\n",
    "\n",
    "    y = np.array([point_in_polygon(pt, corners) for pt in X]).astype(np.int32)\n",
    "    return X, y, corners\n",
    "\n",
    "def generate_donut_data(n=2000, inner_radius=0.25, outer_radius=0.4):\n",
    "    \"\"\"Generate donut/annulus shape data.\"\"\"\n",
    "    X = np.random.uniform(0, 1, (n, 2))\n",
    "    cx, cy = np.random.uniform(0.45, 0.55, 2)\n",
    "    dist = np.sqrt((X[:,0]-cx)**2 + (X[:,1]-cy)**2)\n",
    "    y = ((dist < outer_radius) & (dist > inner_radius)).astype(np.float32)\n",
    "    return X, y, (cx, cy, inner_radius, outer_radius)\n",
    "\n",
    "def generate_eight_shape_data(n=2000, inner_radius_1=0.15, outer_radius_1=0.25, inner_radius_2=0.2, outer_radius_2=0.3):\n",
    "    \"\"\"Generate figure-eight or double donut shape data.\"\"\"\n",
    "    X = np.random.uniform(0, 1, (n, 2))\n",
    "    cx_1, cy_1 = np.random.uniform(0.3, 0.35, 2)\n",
    "    cx_2, cy_2 = np.random.uniform(0.6, 0.65, 2)\n",
    "    dist_1 = np.sqrt((X[:,0]-cx_1)**2 + (X[:,1]-cy_1)**2)\n",
    "    dist_2 = np.sqrt((X[:,0]-cx_2)**2 + (X[:,1]-cy_2)**2)\n",
    "    y = (((dist_1 < outer_radius_1) & (dist_1 > inner_radius_1)) | ((dist_2 < outer_radius_2) & (dist_2 > inner_radius_2))).astype(np.float32)\n",
    "    return X, y, (cx_1, cy_1, inner_radius_1, outer_radius_1, cx_2, cy_2, inner_radius_2, outer_radius_2)\n",
    "\n",
    "def generate_n_star_data(n=2000, n_corners=5, inner_radius=0.25, outer_radius=0.4):\n",
    "    \"\"\"Create n-pointed star shape data by defining outer and inner corners.\n",
    "    \n",
    "    Args:\n",
    "        n: Number of data points\n",
    "        n_corners: Number of star points\n",
    "        inner_radius: Radius of inner vertices\n",
    "        outer_radius: Radius of outer vertices (star tips)\n",
    "    \"\"\"\n",
    "    angles = np.linspace(0, 2 * np.pi, n_corners, endpoint=False)\n",
    "    # Adding some noise to make it less regular\n",
    "    angles += np.random.uniform(-0.1, 0.1, size=n_corners)\n",
    "    outer_corners = np.c_[np.cos(angles), np.sin(angles)] * outer_radius\n",
    "    inner_corners = np.c_[np.cos(angles + np.pi / n_corners), np.sin(angles + np.pi / n_corners)] * inner_radius\n",
    "    corners = []\n",
    "    for i in range(n_corners):\n",
    "        corners.append((outer_corners[i,0]+0.5, outer_corners[i,1]+0.5))\n",
    "        corners.append((inner_corners[i,0]+0.5, inner_corners[i,1]+0.5))\n",
    "    return generate_polygon_data(n, corners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Soft Ellipse RBF Layer\n",
    "\n",
    "The core neural network layer implementing soft elliptical radial basis functions with dynamic unit management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftEllipseRBFLayer(Layer):\n",
    "    \"\"\"Custom Keras layer implementing Soft Elliptical Radial Basis Functions.\n",
    "    \n",
    "    This layer can dynamically add RBF units during training, starting from 0 units\n",
    "    and incrementally growing based on training needs.\n",
    "    \n",
    "    Each RBF unit is defined by:\n",
    "    - Center: (x, y) position\n",
    "    - Semi-axes: (a, b) controlling ellipse dimensions\n",
    "    - Angle: θ controlling ellipse rotation\n",
    "    \n",
    "    The activation function is: φ(x) = sigmoid(K * (1 - ||R*(x-c)||/axes))\n",
    "    where R is the rotation matrix and K controls softness.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_units=BASE_MAX_UNITS+ECR_MAX_UNITS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_units = max_units\n",
    "        self.num_units = tf.Variable(tf.constant(0, dtype=tf.int32), trainable=False, name=\"num_units\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Pre-allocate maximum space for all parameters\n",
    "        self.centers = self.add_weight(name=\"centers\",\n",
    "            shape=(self.max_units, 2),\n",
    "            initializer=\"zeros\",\n",
    "            constraint=tf.keras.constraints.MinMaxNorm(min_value=-2.0, max_value=2.0),\n",
    "            trainable=True)\n",
    "\n",
    "        self.axes = self.add_weight(name=\"axes\",\n",
    "            shape=(self.max_units, 2),\n",
    "            initializer=keras.initializers.Constant(0.1),\n",
    "            regularizer=keras.regularizers.l2(1e-5),\n",
    "            constraint=tf.keras.constraints.MinMaxNorm(min_value=0.02, max_value=2.0),\n",
    "            trainable=True)\n",
    "\n",
    "        self.angles = self.add_weight(name=\"angles\",\n",
    "            shape=(self.max_units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True)\n",
    "\n",
    "    def add_unit(self, center, axes, angle, log=no_op):\n",
    "        \"\"\"Add a new RBF unit by updating the next available slot.\n",
    "        \n",
    "        Args:\n",
    "            center: (x, y) center coordinates\n",
    "            axes: (a, b) semi-axis lengths\n",
    "            angle: rotation angle in radians\n",
    "            log: optional logging function\n",
    "        \"\"\"\n",
    "        if log:\n",
    "            log(f\"add unit: center={center}, axes(a,b)={axes}, angle(a)={np.degrees(angle):.1f}°\")\n",
    "        current_units = int(self.num_units.numpy())\n",
    "        if current_units >= self.max_units:\n",
    "            raise ValueError(f\"Cannot add more units. Maximum is {self.max_units}\")\n",
    "        \n",
    "        # Update the parameters at the current index\n",
    "        self.centers[current_units:current_units+1].assign(center[None])\n",
    "        self.axes[current_units:current_units+1].assign(axes[None])\n",
    "        self.angles[current_units:current_units+1].assign([angle])\n",
    "        \n",
    "        # Increment the unit counter\n",
    "        self.num_units.assign_add(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass computing RBF activations for all active units.\"\"\"\n",
    "        n = self.num_units\n",
    "\n",
    "        def no_units():\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            return tf.zeros((batch_size, 0), dtype=tf.float32)\n",
    "\n",
    "        def some_units():\n",
    "            centers = self.centers[:n]\n",
    "            axes    = self.axes[:n]\n",
    "            angles  = self.angles[:n]\n",
    "\n",
    "            # Compute distance from input to each center\n",
    "            diff = tf.expand_dims(tf.cast(inputs, tf.float32), 1) - centers\n",
    "            \n",
    "            # Apply rotation transformation\n",
    "            c, s = tf.cos(angles), tf.sin(angles)\n",
    "            R = tf.stack([\n",
    "                tf.stack([c, -s], axis=-1),\n",
    "                tf.stack([s,  c], axis=-1)\n",
    "            ], axis=-2)\n",
    "\n",
    "            diff_rot = tf.einsum('bij,ijk->bik', diff, R)\n",
    "            \n",
    "            # Add numerical stability: ensure axes are not too small and add epsilon\n",
    "            stable_axes = tf.maximum(axes, 1e-4)\n",
    "            eps = 1e-8\n",
    "            stable_diff_rot = diff_rot + tf.sign(diff_rot) * eps\n",
    "            \n",
    "            # Compute normalized distance (elliptical)\n",
    "            norm = stable_diff_rot / stable_axes\n",
    "            tf.debugging.check_numerics(norm, \"norm contains NaN or Inf!\")\n",
    "            d = tf.norm(norm, axis=-1)\n",
    "            tf.debugging.check_numerics(d, \"d contains NaN or Inf!\")\n",
    "            \n",
    "            # Clamp d to prevent extreme values\n",
    "            d = tf.clip_by_value(d, 1e-8, 1e4)\n",
    "            \n",
    "            # Apply soft sigmoid activation\n",
    "            phi = tf.sigmoid(K_SIGMOID * (1.0 - d))\n",
    "            tf.debugging.check_numerics(phi, \"phi contains NaN or Inf!\")\n",
    "            return phi\n",
    "\n",
    "        return tf.cond(tf.equal(n, 0), no_units, some_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Incremental Model Architecture\n",
    "\n",
    "Complete model architecture that combines the Soft Ellipse RBF layer with a dense output layer, supporting incremental unit addition during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalModel(tf.keras.Model):\n",
    "    \"\"\"Neural network model with incremental RBF unit addition capability.\n",
    "    \n",
    "    The model consists of:\n",
    "    1. SoftEllipseRBFLayer: Produces RBF activations\n",
    "    2. Dense layer: Linear combination of RBF outputs → sigmoid → probability\n",
    "    \n",
    "    Key features:\n",
    "    - Starts with 0 RBF units\n",
    "    - Units can be added dynamically during training\n",
    "    - Dense layer weights are initialized to handle variable RBF units\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, max_units=BASE_MAX_UNITS+ECR_MAX_UNITS):\n",
    "        assert 0 < max_units <= BASE_MAX_UNITS+ECR_MAX_UNITS\n",
    "        super().__init__()\n",
    "        self.max_units = max_units\n",
    "        self.rbf = SoftEllipseRBFLayer(max_units=max_units)\n",
    "        self.dense = Dense(1, activation=\"sigmoid\")\n",
    "        \n",
    "        # Build the dense layer to initialize weights\n",
    "        self.dense.build((None, max_units))\n",
    "        \n",
    "        # Initialize dense layer for 0 active units\n",
    "        initial_weights = np.zeros((max_units, 1))\n",
    "        initial_bias = np.array([-2.0])  # Bias towards negative prediction initially\n",
    "        self.dense.set_weights([initial_weights, initial_bias])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass with dynamic padding for variable RBF units.\"\"\"\n",
    "        rbf_output = self.rbf(inputs)\n",
    "        n_active = self.rbf.num_units\n",
    "\n",
    "        def pad_and_dense():\n",
    "            # Pad RBF output to match dense layer input size\n",
    "            batch_size = tf.shape(rbf_output)[0]\n",
    "            padding_size = self.max_units - n_active\n",
    "            padding = tf.zeros((batch_size, padding_size))\n",
    "            padded_rbf_output = tf.concat([rbf_output, padding], axis=1)\n",
    "            return self.dense(padded_rbf_output)\n",
    "        \n",
    "        def zero_dense():\n",
    "            # Handle case with no active RBF units\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            padded_rbf_output = tf.zeros((batch_size, self.max_units))\n",
    "            return self.dense(padded_rbf_output)\n",
    "        \n",
    "        return tf.cond(tf.equal(n_active, 0), zero_dense, pad_and_dense)\n",
    "\n",
    "    def add_unit(self, center, axes, angle, init_weight=0.1, log=no_op):\n",
    "        \"\"\"Add new RBF unit and initialize its corresponding dense layer weight.\n",
    "        \n",
    "        Args:\n",
    "            center: RBF center coordinates\n",
    "            axes: RBF semi-axis lengths\n",
    "            angle: RBF rotation angle\n",
    "            init_weight: Initial weight for the new unit in dense layer\n",
    "            log: Optional logging function\n",
    "        \"\"\"\n",
    "        # Add unit to RBF layer\n",
    "        self.rbf.add_unit(center, axes, angle, log=log)\n",
    "        \n",
    "        # Set the weight for the new unit in the dense layer\n",
    "        current_units = int(self.rbf.num_units.numpy()) - 1 \n",
    "        self.dense.kernel[current_units:current_units+1].assign([[init_weight]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 4. Training Strategy\n",
    "\n",
    "### 4.1 Pruning and DBSCAN-based Clustering\n",
    "\n",
    "After each unit is added and the resulting model re-trained, some units may have minimal impact.  \n",
    "We prune those whose **ellipse area × absolute weight** falls below a threshold, ensuring only significant units remain.  \n",
    "\n",
    "Strategic placement of new RBF units is guided by clustering analysis using density‑based clustering via DBSCAN to find the largest false positive/negative clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_small_units(model, threshold=PRUNE_BUMP_VOLUME_THRESHOLD, log=None):\n",
    "    \"\"\"Prune RBF units whose area × |weight| < threshold.\n",
    "    \n",
    "    The importance of an RBF unit is measured by the product of:\n",
    "    - Its elliptical area: π × a × b\n",
    "    - The absolute value of its output weight\n",
    "    \n",
    "    Units with low importance are removed to prevent overfitting and\n",
    "    improve model interpretability.\n",
    "    \n",
    "    Args:\n",
    "        model: IncrementalModel instance\n",
    "        threshold: Minimum importance threshold\n",
    "        log: Optional logging function\n",
    "        \n",
    "    Returns:\n",
    "        Number of units removed\n",
    "    \"\"\"\n",
    "    rbf_layer = model.rbf\n",
    "    dense_layer = model.dense\n",
    "    \n",
    "    n_active = int(rbf_layer.num_units.numpy())\n",
    "    if n_active == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Extract parameters for active units only\n",
    "    active_centers = rbf_layer.centers[:n_active].numpy()\n",
    "    active_axes = rbf_layer.axes[:n_active].numpy()\n",
    "    active_angles = rbf_layer.angles[:n_active].numpy()\n",
    "    active_weights = dense_layer.kernel[:n_active].numpy().flatten()\n",
    "\n",
    "    # Compute elliptical area for each unit\n",
    "    areas = np.pi * active_axes[:, 0] * active_axes[:, 1]\n",
    "\n",
    "    # Calculate importance as area × |weight|\n",
    "    importance = areas * np.abs(active_weights)\n",
    "\n",
    "    # Determine which units to keep\n",
    "    keep_indices = importance >= threshold\n",
    "    n_keep = np.sum(keep_indices)\n",
    "    \n",
    "    if n_keep > 0 and n_keep < n_active:\n",
    "        if log:\n",
    "            log(f\"Pruning {n_active - n_keep} small units out of {n_active}.\")\n",
    "\n",
    "        # Keep only the selected active units\n",
    "        kept_centers = active_centers[keep_indices]\n",
    "        kept_axes = active_axes[keep_indices]\n",
    "        kept_angles = active_angles[keep_indices]\n",
    "        kept_weights = active_weights[keep_indices]\n",
    "        \n",
    "        # Reset all parameters to default values\n",
    "        rbf_layer.centers.assign(tf.zeros_like(rbf_layer.centers))\n",
    "        rbf_layer.axes.assign(tf.constant(0.1, shape=rbf_layer.axes.shape))\n",
    "        rbf_layer.angles.assign(tf.zeros_like(rbf_layer.angles))\n",
    "        dense_layer.kernel.assign(tf.zeros_like(dense_layer.kernel))\n",
    "        \n",
    "        # Assign the kept units to the beginning of parameter arrays\n",
    "        rbf_layer.centers[:n_keep].assign(kept_centers)\n",
    "        rbf_layer.axes[:n_keep].assign(kept_axes)\n",
    "        rbf_layer.angles[:n_keep].assign(kept_angles)\n",
    "        dense_layer.kernel[:n_keep].assign(kept_weights.reshape(-1, 1))\n",
    "        \n",
    "        # Update the number of active units\n",
    "        rbf_layer.num_units.assign(n_keep)\n",
    "    elif n_keep == 0:\n",
    "        if log:\n",
    "            log(f\"All {n_active} units would be pruned! Keeping all units.\")\n",
    "        return 0\n",
    "    \n",
    "    return n_active - n_keep\n",
    "\n",
    "def guess_dbscan_params(n_false, n_true, n_sample,\n",
    "                        min_min_samples=4, max_min_samples=20,\n",
    "                        eps_min=1e-2, eps_max=1.0):\n",
    "    \"\"\"Estimate optimal DBSCAN parameters based on data characteristics.\n",
    "    \n",
    "    Args:\n",
    "        n_false: Number of false prediction points\n",
    "        n_true: Number of true prediction points \n",
    "        n_sample: Total sample size\n",
    "        \n",
    "    Returns:\n",
    "        eps: Neighborhood radius for DBSCAN\n",
    "        min_samples: Minimum points required to form a cluster\n",
    "    \"\"\"\n",
    "    if n_false <= 0:\n",
    "        return 0.0, 0\n",
    "\n",
    "    # Rule-of-thumb: min_samples scales with sqrt of false points\n",
    "    min_samples = int(np.clip(round(np.sqrt(n_false)), min_min_samples, max_min_samples))\n",
    "    \n",
    "    # Effective area occupied by false points\n",
    "    A_eff = (n_false + n_true) / n_sample\n",
    "    A_eff = np.clip(A_eff, 0.01, 0.5)\n",
    "\n",
    "    # Estimate eps based on expected cluster density\n",
    "    beta = 1.2  # Scale factor for cluster connectivity\n",
    "    eps = np.sqrt(min_samples * A_eff / (np.pi * n_false)) * beta\n",
    "    eps = np.clip(eps, eps_min, eps_max)\n",
    "\n",
    "    return eps, min_samples\n",
    "\n",
    "def get_max_cluster(false_points, fn_fp=\"\", eps=0.1, min_samples=10, log=no_op):\n",
    "    \"\"\"Use DBSCAN to find the largest cluster in false prediction points.\n",
    "    \n",
    "    Args:\n",
    "        false_points: Points where model predictions are incorrect\n",
    "        fn_fp: Label for logging (\"FN\" or \"FP\")\n",
    "        eps: DBSCAN neighborhood radius\n",
    "        min_samples: Minimum cluster size\n",
    "        log: Logging function\n",
    "        \n",
    "    Returns:\n",
    "        cluster_size: Size of the largest cluster\n",
    "        cluster_points: Points belonging to the largest cluster\n",
    "    \"\"\"\n",
    "    labels_false = DBSCAN(eps=eps, min_samples=min_samples).fit_predict(false_points)\n",
    "\n",
    "    size_noise = (labels_false == -1).sum()\n",
    "    # Drop noise points (labeled as -1)\n",
    "    mask = labels_false != -1\n",
    "    filtered_false_points = false_points[mask]\n",
    "    filtered_false_labels = labels_false[mask]\n",
    "\n",
    "    cluster_points = []\n",
    "    cluster_size = 0\n",
    "\n",
    "    unique_clusters = set(filtered_false_labels)\n",
    "    if len(unique_clusters) >= 1:\n",
    "        if log:\n",
    "            log(len(unique_clusters), f\"false {fn_fp} clusters found (noises dropped: {size_noise})\")\n",
    "            for cluster_id in unique_clusters:\n",
    "                log(\"False Cluster ID:\", cluster_id,\n",
    "                      \"size:\", np.sum(filtered_false_labels == cluster_id))\n",
    "\n",
    "        # Select the largest cluster\n",
    "        most_important_cluster = max(\n",
    "            unique_clusters,\n",
    "            key=lambda x: np.sum(filtered_false_labels == x)\n",
    "        )\n",
    "        cluster_points = filtered_false_points[filtered_false_labels == most_important_cluster]\n",
    "        cluster_size = len(cluster_points)\n",
    "\n",
    "    return cluster_size, cluster_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Base Stage: Initial Unit Placement\n",
    "\n",
    "The base stage incrementally adds RBF units by identifying clusters of false negative predictions and placing units at their medoids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_stage_incremental_training(model, X_train, y_train, X_val, y_val, cb, log=no_op):\n",
    "    \"\"\"Base stage: Incrementally add RBF units targeting false negative clusters.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Identify false negative predictions\n",
    "    2. Cluster false negatives using DBSCAN\n",
    "    3. Place new RBF unit at largest cluster's medoid\n",
    "    4. Train model and repeat until IoU target is reached\n",
    "    \n",
    "    Args:\n",
    "        model: IncrementalModel instance\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data (can be None)\n",
    "        cb: List of Keras callbacks\n",
    "        log: Logging function\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        iou: Final IoU score\n",
    "    \"\"\"\n",
    "    n_samples = X_train.shape[0]\n",
    "    iou = 0.0\n",
    "    n_in, n_out = np.bincount(y_train.astype(int))\n",
    "    n_real_hits = 0 \n",
    "\n",
    "    while int(model.rbf.num_units.numpy()) < BASE_MAX_UNITS:\n",
    "        n_real_hits += 1\n",
    "        if n_real_hits > BASE_MAX_UNITS * 2:\n",
    "            log(\"Too many iterations without improvement, stopping.\")\n",
    "            break\n",
    "\n",
    "        # Identify false negatives (model predicts 0, true label is 1)\n",
    "        y_pred = (model.predict(X_train, verbose=0).reshape(-1) > 0.5).astype(int)\n",
    "        false_neg = X_train[(y_pred==0) & (y_train==1)]\n",
    "        if len(false_neg) == 0:\n",
    "            log(\"No false negatives left, stopping unit addition.\")\n",
    "            break\n",
    "\n",
    "        n_false_neg = len(false_neg)\n",
    "        n_true_neg = n_out - n_false_neg\n",
    "        log(f\"False negative count: {n_false_neg} | True negative count: {n_true_neg}\")\n",
    "        \n",
    "        # Estimate DBSCAN parameters and find largest cluster\n",
    "        eps_FN, min_FN = guess_dbscan_params(n_false_neg, n_true_neg, n_samples)\n",
    "        log(f\"DBSCAN params for FN: eps={eps_FN:.4f}, min_samples={min_FN}\")\n",
    "        size_FN, points_FN = get_max_cluster(false_neg, fn_fp=\"FN\", eps=eps_FN, min_samples=min_FN, log=log)\n",
    "        log(f\"False negative size: {size_FN}\")\n",
    "\n",
    "        # Check if we have a valid cluster before proceeding\n",
    "        if size_FN == 0:\n",
    "            log(\"No false negative cluster found, skipping unit addition.\")\n",
    "            continue\n",
    "\n",
    "        # Place new RBF unit at cluster medoid\n",
    "        centroid = fast_medoid(points_FN)\n",
    "        log(f\"Chosen centroid for new unit: {centroid}\")\n",
    "        \n",
    "        # Initialize with small, roughly circular ellipse\n",
    "        a = np.random.uniform(0.1, 0.15)\n",
    "        b = np.random.uniform(0.1, 0.15)\n",
    "        model.add_unit(centroid, np.array([a,b]), np.random.uniform(0, np.pi), init_weight=2, log=log)\n",
    "          \n",
    "        # Train model with new unit\n",
    "        model.fit(X_train, y_train, \n",
    "                  batch_size=BASE_BATCH,\n",
    "                  epochs=BASE_EPOCHS, \n",
    "                  validation_data=(X_val, y_val) if X_val is not None else None, \n",
    "                  callbacks=cb, \n",
    "                  verbose=0\n",
    "                  )\n",
    "                  \n",
    "        # Log current model state\n",
    "        n_active = int(model.rbf.num_units.numpy())\n",
    "        log(\"centers:\", model.rbf.centers[:n_active].numpy())\n",
    "        log(\"axes:\", model.rbf.axes[:n_active].numpy())\n",
    "        log(\"weights:\", model.dense.kernel[:n_active].numpy().T)\n",
    "\n",
    "        # Check if IoU target reached\n",
    "        if X_val is not None:\n",
    "            iou = compute_iou(model, X_val, y_val)\n",
    "        else:\n",
    "            iou = compute_iou(model, X_train, y_train)\n",
    "\n",
    "        if iou >= BASE_IOU_TARGET:\n",
    "            log(f\"Base Stage reached IoU target of {BASE_IOU_TARGET*100:.1f}%, stopping stage early.\")\n",
    "            break\n",
    "        \n",
    "        # Remove insignificant units\n",
    "        prune_small_units(model, log=log)\n",
    "\n",
    "    # Final base stage training\n",
    "    model.fit(X_train, y_train, \n",
    "                batch_size=ECR_BATCH,\n",
    "                epochs=BASE_EPOCHS, \n",
    "                validation_data=(X_val, y_val) if X_val is not None else None, \n",
    "                callbacks=cb, \n",
    "                verbose=0\n",
    "                )\n",
    "    \n",
    "    # Final IoU computation\n",
    "    if X_val is not None:\n",
    "        iou = compute_iou(model, X_val, y_val)\n",
    "    else:\n",
    "        iou = compute_iou(model, X_train, y_train)\n",
    "\n",
    "    return model, iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ECR Stage: Error Cluster Refinement\n",
    "\n",
    "The ECR (Error Cluster Refinement) stage addresses remaining prediction errors by analyzing both false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecr_stage_incremental_training(model, X_train, y_train, X_val, y_val, cb, log=no_op):\n",
    "    \"\"\"ECR stage: Refine model by addressing both false positives and negatives.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Analyze both false positive and false negative clusters\n",
    "    2. Select most significant error cluster\n",
    "    3. Add RBF unit with appropriate weight sign:\n",
    "       - Negative weight for false positive regions\n",
    "       - Positive weight for false negative regions\n",
    "    4. Size RBF unit based on cluster spread\n",
    "    \n",
    "    Args:\n",
    "        model: IncrementalModel from base stage\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data (can be None)\n",
    "        cb: List of Keras callbacks\n",
    "        log: Logging function\n",
    "        \n",
    "    Returns:\n",
    "        model: Refined model\n",
    "        iou: Final IoU score\n",
    "    \"\"\"\n",
    "    n_samples = X_train.shape[0]\n",
    "    iou = 0.0\n",
    "    n_in, n_out = np.bincount(y_train.astype(int))\n",
    "    n_real_hits = 0\n",
    "\n",
    "    while int(model.rbf.num_units.numpy()) < BASE_MAX_UNITS + ECR_MAX_UNITS:\n",
    "        n_real_hits += 1\n",
    "        if n_real_hits > ECR_MAX_UNITS * 2:\n",
    "            log(\"Too many extra iterations without improvement, stopping.\")\n",
    "            break\n",
    "\n",
    "        y_pred = (model.predict(X_train, verbose=0).reshape(-1) > 0.5).astype(int)\n",
    "\n",
    "        # Analyze false positive clusters (model predicts 1, true label is 0)\n",
    "        false_pos = X_train[(y_pred==1) & (y_train==0)]\n",
    "        n_false_pos = len(false_pos)\n",
    "        n_true_pos = n_in - n_false_pos\n",
    "        log(\"False positive count:\", n_false_pos, \"| True positive count:\", n_true_pos)\n",
    "        eps_FP, min_FP = guess_dbscan_params(n_false_pos, n_true_pos, n_samples)\n",
    "        size_FP, points_FP = get_max_cluster(false_pos, fn_fp=\"FP\", eps=eps_FP, min_samples=min_FP, log=log)\n",
    "\n",
    "        # Analyze false negative clusters\n",
    "        false_neg = X_train[(y_pred==0) & (y_train==1)]\n",
    "        n_false_neg = len(false_neg)\n",
    "        n_true_neg = n_out - n_false_neg\n",
    "        log(\"False negative count:\", n_false_neg, \"| True negative count:\", n_true_neg)\n",
    "        eps_FN, min_FN = guess_dbscan_params(n_false_neg, n_true_neg, n_samples)\n",
    "        size_FN, points_FN = get_max_cluster(false_neg, fn_fp=\"FN\", eps=eps_FN, min_samples=min_FN, log=log)\n",
    "\n",
    "        if size_FP == 0 and size_FN == 0:\n",
    "            log(\"No more clusters found. Stopping extra fine-tuning.\")\n",
    "            break\n",
    "\n",
    "        # Select most significant cluster based on relative error rates\n",
    "        if size_FP/n_out >= size_FN/n_in and size_FP > 0:\n",
    "            log(f\"Most important cluster is FP with size {size_FP} (eps={eps_FP:.3f}, min_samples={min_FP})\")\n",
    "            init_weight = -1  # Negative weight to suppress false positives\n",
    "            cluster_points = points_FP\n",
    "            cluster_size = size_FP\n",
    "        else:\n",
    "            log(f\"Most important cluster is FN with size {size_FN} (eps={eps_FN:.3f}, min_samples={min_FN})\")   \n",
    "            init_weight = +1  # Positive weight to enhance false negatives\n",
    "            cluster_points = points_FN\n",
    "            cluster_size = size_FN\n",
    "\n",
    "        # Safety check: ensure we have a valid cluster\n",
    "        if cluster_size == 0 or len(cluster_points) == 0:\n",
    "            if log:\n",
    "                log(\"Selected cluster is empty, skipping unit addition.\")\n",
    "            continue\n",
    "\n",
    "        # Scale weight and add noise for robustness\n",
    "        beta_factor = 2.0\n",
    "        init_weight = init_weight * np.random.uniform(0.95, 1.05) * beta_factor\n",
    "        centroid = fast_medoid(cluster_points) * np.random.uniform(0.98, 1.02)\n",
    "\n",
    "        # Size RBF unit based on cluster spread\n",
    "        cluster_min = cluster_points.min(axis=0)\n",
    "        cluster_max = cluster_points.max(axis=0)\n",
    "\n",
    "        gamma_factor = 0.2  # Fraction of cluster span for RBF size\n",
    "        a = gamma_factor * (cluster_max[0] - cluster_min[0]) * np.random.uniform(0.98, 1.02)\n",
    "        b = gamma_factor * (cluster_max[1] - cluster_min[1]) * np.random.uniform(0.98, 1.02)\n",
    "\n",
    "        log(f\"adding unit for false cluster: center({centroid[0]:.2f}, {centroid[1]:.2f}) (a={a:.2f}, b={b:.2f}) - n_size = {len(cluster_points)} weight={init_weight:.2f}\")\n",
    "        model.add_unit(centroid, np.array([a,b]), 0, init_weight=init_weight, log=log)\n",
    "\n",
    "        # Train model with new unit\n",
    "        model.fit(X_train, y_train, \n",
    "                  batch_size=ECR_BATCH,\n",
    "                  epochs=ECR_EPOCHS, \n",
    "                  validation_data=(X_val, y_val) if X_val is not None else None, \n",
    "                  callbacks=cb, \n",
    "                  verbose=0\n",
    "                  )\n",
    "\n",
    "        # Check if ECR IoU target reached\n",
    "        if X_val is not None:\n",
    "            iou = compute_iou(model, X_val, y_val)\n",
    "        else:\n",
    "            iou = compute_iou(model, X_train, y_train)\n",
    "\n",
    "        if iou >= ECR_IOU_TARGET:\n",
    "            log(f\"ECR Stage reached IoU target of {ECR_IOU_TARGET*100:.1f}%, stopping stage early.\")\n",
    "            break\n",
    "\n",
    "        # Remove insignificant units\n",
    "        prune_small_units(model, log=log)\n",
    "\n",
    "    # Final ECR stage training\n",
    "    model.fit(X_train, y_train, \n",
    "                batch_size=FINAL_BATCH,\n",
    "                epochs=ECR_EPOCHS, \n",
    "                validation_data=(X_val, y_val) if X_val is not None else None, \n",
    "                callbacks=cb, \n",
    "                verbose=0\n",
    "                )\n",
    "\n",
    "    # Final IoU computation\n",
    "    if X_val is not None:\n",
    "        iou = compute_iou(model, X_val, y_val)\n",
    "    else:\n",
    "        iou = compute_iou(model, X_train, y_train)\n",
    "        \n",
    "    return model, iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 5. Training Pipeline & Visualization\n",
    "\n",
    "Comprehensive training pipeline with real-time visualization of the learning process, including training progress, RBF unit evolution, and decision boundary formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Custom Keras callback for real-time training visualization.\n",
    "    \n",
    "    Displays:\n",
    "    - Training data with current RBF ellipses\n",
    "    - Decision boundary evolution\n",
    "    - Training metrics and progress\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, ax1, ax2, out_plots, stage_info=\"\", interval=2, log=no_op):\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.ax1 = ax1  # For training progress\n",
    "        self.ax2 = ax2  # For decision boundary\n",
    "        self.out_plots = out_plots\n",
    "        self.stage_info = stage_info\n",
    "        self.interval = interval\n",
    "        self.log = log\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get total epochs from params\n",
    "        total_epochs = self.params.get('epochs', -1)\n",
    "        \n",
    "        # Show every interval epochs OR the last epoch\n",
    "        is_last_epoch = (epoch + 1) == total_epochs\n",
    "        if epoch % self.interval != 0 and not is_last_epoch:\n",
    "            return\n",
    "\n",
    "        # Clear both axes\n",
    "        self.ax1.clear()\n",
    "        self.ax2.clear()\n",
    "        \n",
    "        n_active = int(self.model.rbf.num_units.numpy())\n",
    "\n",
    "        # Plot training data on ax1 with ellipses\n",
    "        self.ax1.scatter(self.X_train[:,0], self.X_train[:,1], c=self.y_train, cmap=\"bwr\", edgecolor=\"k\", s=14, alpha=0.6)\n",
    "\n",
    "        # Plot RBF ellipses on ax1 with color coding by weight\n",
    "        if n_active > 0:\n",
    "            active_centers = self.model.rbf.centers[:n_active].numpy()\n",
    "            active_axes = self.model.rbf.axes[:n_active].numpy()\n",
    "            active_angles = self.model.rbf.angles[:n_active].numpy()\n",
    "            active_weights = self.model.dense.kernel[:n_active].numpy().flatten()\n",
    "            \n",
    "            for center, axes, angle, w in zip(active_centers, active_axes, active_angles, active_weights):\n",
    "                color = \"orange\" if w > 0 else \"cyan\"  # Orange for positive, cyan for negative weight\n",
    "                e = patches.Ellipse(\n",
    "                    xy=center,\n",
    "                    width=2*axes[0],\n",
    "                    height=2*axes[1],\n",
    "                    angle=np.degrees(angle),\n",
    "                    fill=False,\n",
    "                    edgecolor=color,\n",
    "                    lw=2.5\n",
    "                )\n",
    "                self.ax1.add_patch(e)\n",
    "\n",
    "        # Plot decision boundary on ax2\n",
    "        plot_decision_boundary(self.model, \n",
    "                               self.X_train, self.y_train, \n",
    "                               self.ax2, \n",
    "                               grid_resolution=100, \n",
    "                               title=f\"Training Decision Boundary\\n(seed={RndSeed.get_seed()})\"\n",
    "                               )\n",
    "\n",
    "        # Get and display metrics\n",
    "        train_acc = logs.get('binary_accuracy', 0) if logs else 0\n",
    "        train_iou = compute_iou(self.model, self.X_train, self.y_train)\n",
    "\n",
    "        # Set descriptive titles\n",
    "        epoch_info = f\"Epoch {epoch+1}/{total_epochs}\" if total_epochs > 0 else f\"Epoch {epoch+1}\"\n",
    "        self.ax1.set_title(f\"{self.stage_info} | Units={n_active} | {epoch_info}\\ntrain_acc={train_acc:.4f} | train_iou={train_iou*100:.1f}%\")\n",
    "        \n",
    "        self.ax1.set_xlim(0, 1)\n",
    "        self.ax1.set_ylim(0, 1)\n",
    "        \n",
    "        # Update display\n",
    "        fig = self.ax1.figure\n",
    "        if self.out_plots:\n",
    "            with self.out_plots:\n",
    "                clear_output(wait=True)\n",
    "                display(fig)\n",
    "        else:\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "class VerboseEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    \"\"\"Enhanced EarlyStopping with detailed logging.\"\"\"\n",
    "    \n",
    "    def __init__(self, log=no_op, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.log = log\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super().on_train_end(logs)\n",
    "        if self.stopped_epoch > 0 and self.log:\n",
    "            self.log(f\"\\n🛑 Training stopped at epoch {self.stopped_epoch+1}.\")\n",
    "            self.log(f\"Reason: No improvement in '{self.monitor}' for {self.patience} epochs \"\n",
    "                     f\"(min_delta={self.min_delta}). Best val_binary_accuracy: {self.best:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(X, y, b_validation=True, plot_frame_interval=3, verbose=0):\n",
    "    \"\"\"Complete training pipeline for Soft Ellipse RBF model.\n",
    "    \n",
    "    Executes the three-stage training process:\n",
    "    1. Base Stage: Initial RBF placement targeting false negatives\n",
    "    2. ECR Stage: Error cluster refinement for both FP and FN\n",
    "    3. Final Stage: Fine-tuning with optimized parameters\n",
    "    \n",
    "    Args:\n",
    "        X, y: Dataset for training\n",
    "        b_validation: Whether to use train/validation split\n",
    "        plot_frame_interval: Epochs between visualization updates\n",
    "        verbose: Logging verbosity (0=silent, 1=full)\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained IncrementalModel\n",
    "        iou: Final IoU score\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data (or None)\n",
    "    \"\"\"\n",
    "\n",
    "    # Data splitting\n",
    "    if b_validation:\n",
    "        test_size = TEST_SIZE\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, stratify=y, random_state=RndSeed.get_seed())\n",
    "    else:\n",
    "        test_size = 0.0\n",
    "        X_train, y_train = X, y\n",
    "        X_val, y_val = None, None\n",
    "\n",
    "    # Setup visualization\n",
    "    plt.style.use('default')\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    ax1.set_aspect('equal')\n",
    "    ax2.set_aspect('equal')\n",
    "    ax3.set_aspect('equal')\n",
    "    ax3.set_visible(False)  # Hide third subplot initially\n",
    "\n",
    "    # Initial data visualization\n",
    "    ax1.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=\"bwr\", edgecolor=\"k\", s=14, alpha=0.6)\n",
    "    ax1.set_title(\"Training Data\")\n",
    "    ax2.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=\"bwr\", edgecolor=\"k\", s=14, alpha=0.6)\n",
    "    ax2.set_title(\"Training Decision Boundary\")\n",
    "\n",
    "    # Setup logging\n",
    "    out_logs = None\n",
    "    out_plots = None\n",
    "    log = no_op\n",
    "    \n",
    "    if verbose == 1:\n",
    "        out_plots = Output(layout={'border': '0px solid black'})\n",
    "        out_logs = Output(layout={'border': '1px solid black'})\n",
    "        outs = [out_logs, out_plots]\n",
    "        \n",
    "        def log_fn(*args, **kwargs):\n",
    "            with out_logs:\n",
    "                print(*args, **kwargs)\n",
    "        log = log_fn\n",
    "        \n",
    "        log(f\"🎲 Random seed set to: {RndSeed.get_seed()}\\n\")\n",
    "        \n",
    "        with out_plots:\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "        display(VBox(outs, layout={'width': '100%'}))\n",
    "    else:\n",
    "        print(f\"🎲 Random seed set to: {RndSeed.get_seed()}\")\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "\n",
    "    sleep(10)\n",
    "    \n",
    "    # Dataset statistics\n",
    "    n_samples = X.shape[0]\n",
    "    n_out = np.sum(y == 0)  # count of 0s (outside shape)\n",
    "    n_in = np.sum(y == 1)   # count of 1s (inside shape)\n",
    "    log(f\"📊 Dataset: {n_samples} samples = {n_in}(1s) + {n_out}(0s) \"\n",
    "        f\"| proportions: {n_in/n_samples:.2%}(1s) - {n_out/n_samples:.2%}(0s) \"\n",
    "        f\"| training-validation split: {1-test_size:.2f} - {test_size:.2f}\\n\")\n",
    "\n",
    "    # Setup callbacks\n",
    "    plot_cb = PlotCallback(X_train, y_train, ax1, ax2, out_plots=out_plots, \n",
    "                          stage_info=\"Base Stage\", interval=plot_frame_interval, log=log)\n",
    "    es_cb = VerboseEarlyStopping(\n",
    "                    monitor='val_binary_accuracy',\n",
    "                    patience=BASE_PATIENCE,\n",
    "                    min_delta=BASE_MIN_DELTA,\n",
    "                    mode='max',\n",
    "                    restore_best_weights=True,\n",
    "                    log=log\n",
    "                )\n",
    "    cb = [plot_cb, es_cb]\n",
    "\n",
    "    # 🔥 BASE STAGE\n",
    "    log(\"-\" * 50)\n",
    "    log(\"🔥 Starting Base Stage...\\n\")\n",
    "    model = IncrementalModel(input_dim=X_train.shape[1], max_units=BASE_MAX_UNITS+ECR_MAX_UNITS)\n",
    "    model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(BASE_ADAM),\n",
    "            loss=\"mse\",\n",
    "            # loss=\"binary_crossentropy\",\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "            )    \n",
    "\n",
    "    model, val_iou = base_stage_incremental_training(model, X_train, y_train, X_val, y_val, cb, log=log)\n",
    "    log(f\"\\n✅ Base stage ended with: units={int(model.rbf.num_units.numpy())}, val_iou={val_iou*100:.2f}% \\n\")\n",
    "\n",
    "    # 🔧 ECR STAGE\n",
    "    log(\"-\" * 50)\n",
    "    log(\"🔧 Starting ECR Stage...\\n\")\n",
    "    model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(ECR_ADAM),\n",
    "            loss=\"mse\",\n",
    "            # loss=\"binary_crossentropy\",\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "            )\n",
    "\n",
    "    es_cb.patience = ECR_PATIENCE\n",
    "    es_cb.min_delta = ECR_MIN_DELTA\n",
    "    plot_cb.stage_info = \"ECR Stage\"\n",
    "    model, val_iou = ecr_stage_incremental_training(model, X_train, y_train, X_val, y_val, cb, log=log)\n",
    "    log(f\"\\n✅ ECR stage ended with: units={int(model.rbf.num_units.numpy())}, val_iou={val_iou*100:.2f}%\")\n",
    "\n",
    "    # 🎯 FINAL STAGE\n",
    "    log(\"🎯 Starting Final Stage fine-tuning...\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(FINAL_ADAM),\n",
    "                loss=\"mse\",\n",
    "                # loss=\"binary_crossentropy\",\n",
    "                metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                )\n",
    "    \n",
    "    plot_cb.stage_info = \"Final Stage fine-tuning\"\n",
    "    model.fit(X_train, y_train, \n",
    "              batch_size=FINAL_BATCH,\n",
    "              epochs=FINAL_EPOCHS, \n",
    "              validation_data=(X_val, y_val) if X_val is not None else None, \n",
    "              callbacks=[plot_cb], \n",
    "              verbose=0\n",
    "              )\n",
    "    \n",
    "    # Final evaluation\n",
    "    if X_val is not None:\n",
    "        iou = compute_iou(model, X_val, y_val)\n",
    "    else:\n",
    "        iou = compute_iou(model, X_train, y_train)\n",
    "\n",
    "    # Show final results in third subplot\n",
    "    ax3.set_visible(True)\n",
    "    if X_val is not None:\n",
    "        plot_decision_boundary(model, \n",
    "                               X_val, y_val, ax3, \n",
    "                               grid_resolution=150, \n",
    "                               title=f\"Final Validation Decision Boundary | Units={model.rbf.num_units.numpy()}\\n val_iou={iou*100:.1f}%\"\n",
    "                               )\n",
    "    else:\n",
    "        plot_decision_boundary(model, \n",
    "                               X_train, y_train, ax3, \n",
    "                               grid_resolution=150, \n",
    "                               title=f\"Final Training Decision Boundary | Units={model.rbf.num_units.numpy()}\\n train_iou={iou*100:.1f}%\"\n",
    "                               )\n",
    "        \n",
    "    # Final display update\n",
    "    if out_plots:\n",
    "        with out_plots:\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "    else:\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "\n",
    "    # 🏁 FINAL RESULTS SUMMARY\n",
    "    n_active = int(model.rbf.num_units.numpy())\n",
    "    log(\"-\" * 50)\n",
    "    log(\"🏁 TRAINING COMPLETED!\")\n",
    "    log(f\"🎉 Final validation IoU: {iou*100:.1f}%\\n\")\n",
    "    \n",
    "    if n_active > 0:\n",
    "        # Create comprehensive RBF parameters table\n",
    "        log(\"\\n📊 RBF UNITS SUMMARY TABLE:\")\n",
    "        log(\"=\" * 91)\n",
    "        \n",
    "        # Extract final parameters\n",
    "        centers = model.rbf.centers[:n_active].numpy()\n",
    "        axes = model.rbf.axes[:n_active].numpy()\n",
    "        angles_deg = np.degrees(model.rbf.angles[:n_active].numpy())\n",
    "        weights = model.dense.kernel[:n_active].numpy().flatten()\n",
    "        \n",
    "        # Enhanced table header\n",
    "        header = f\"{'Unit':<4} | {'Center (x, y)':<18} | {'Semi-axes (a, b)':<19} | {'∠°(a vs Ox)':<11} | {'Weight':<8}\"\n",
    "        log(header)\n",
    "        log(\"-\" * len(header))\n",
    "        \n",
    "        # Table rows with parameter details\n",
    "        for i in range(n_active):\n",
    "            center_str = f\"({centers[i,0]:.3f}, {centers[i,1]:.3f})\"\n",
    "            axes_str = f\"({axes[i,0]:.3f}, {axes[i,1]:.3f})\"\n",
    "            row = (f\"{i+1:>4} | \"\n",
    "                   f\"{center_str:<18} | \"\n",
    "                   f\"{axes_str:<19} | \"\n",
    "                   f\"{angles_deg[i]:>11.1f} | \"\n",
    "                   f\"{weights[i]:<+8.3f}\")\n",
    "            log(row)\n",
    "        \n",
    "        log(\"=\" * 91)\n",
    "    else:\n",
    "        log(\"\\n⚠️  No RBF units were created during training.\")\n",
    "\n",
    "    # Close the figure to prevent automatic display\n",
    "    # plt.close(fig)\n",
    "\n",
    "    return model, iou, X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 6. Experimental Results\n",
    "\n",
    "Below we showcase how to create different non‑convex shapes, train the incremental model, and visualise the training progress.  \n",
    "\n",
    "Feel free to adjust the number of samples, change the shape definition, or disable validation for faster experiments.  Use `verbose=1` to see live logs, or `0` for a silent run.\n",
    "\n",
    "### 6.1 L-Shape Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RndSeed.set_seed(197)  # Fixed seed for reproducibility\n",
    "\n",
    "# Generate L-shaped polygon data\n",
    "l_shape_corners = [(0.15, 0.15), (0.85, 0.15), (0.85, 0.35), (0.35, 0.35), (0.35, 0.85), (0.15, 0.85)]\n",
    "X_lshape, y_lshape, _ = generate_polygon_data(n=3000, corners=l_shape_corners)\n",
    "\n",
    "model_lshape, iou_lshape, X_train, y_train, X_val, y_val = build_and_train_model(\n",
    "    X_lshape, y_lshape, \n",
    "    b_validation=True, \n",
    "    plot_frame_interval=5, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 S-Shape Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RndSeed.set_seed(197)  # Fixed seed for reproducibility\n",
    "\n",
    "# Generate S-shaped polygon data \n",
    "s_shape_corners = [\n",
    "    (0.1, 0.1), (0.9, 0.1), (0.9, 0.55), (0.25, 0.55), \n",
    "    (0.25, 0.7), (0.9, 0.7), (0.9, 0.85), (0.1, 0.85), \n",
    "    (0.1, 0.4), (0.75, 0.4), (0.75, 0.25), (0.1, 0.25)\n",
    "]\n",
    "X_sshape, y_sshape, _ = generate_polygon_data(n=3000, corners=s_shape_corners)\n",
    "\n",
    "model_sshape, iou_sshape, _, _, _, _ = build_and_train_model(\n",
    "    X_sshape, y_sshape, \n",
    "    b_validation=True, \n",
    "    plot_frame_interval=3, \n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Star Shape Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RndSeed.set_seed(197)  # Fixed seed for reproducibility\n",
    "\n",
    "# Generate 5-pointed star data\n",
    "X_star, y_star, _ = generate_n_star_data(n=3000, n_corners=5, inner_radius=0.22, outer_radius=0.45)\n",
    "\n",
    "model_star, iou_star, _, _, _, _ = build_and_train_model(\n",
    "    X_star, y_star,\n",
    "    b_validation=True, \n",
    "    plot_frame_interval=5, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Donut Shape Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RndSeed.set_seed(197)  # Fixed seed for reproducibility\n",
    "\n",
    "# Generate donut/annulus data\n",
    "X_donut, y_donut, _ = generate_donut_data(n=3000)\n",
    "\n",
    "model_donut, iou_donut, _, _, _, _ = build_and_train_model(\n",
    "    X_donut, y_donut, \n",
    "    b_validation=True, \n",
    "    plot_frame_interval=5, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Figure-Eight Shape Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RndSeed.set_seed(197)  # Fixed seed for reproducibility\n",
    "\n",
    "# Generate figure-eight (double donut) data\n",
    "X_eight, y_eight, _ = generate_eight_shape_data(n=3000)\n",
    "\n",
    "model_eight, iou_eight, _, _, _, _ = build_and_train_model(\n",
    "    X_eight, y_eight, \n",
    "    b_validation=True, \n",
    "    plot_frame_interval=5, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 7. Interactive Drawing Canvas\n",
    "\n",
    "**Note:** The interactive canvas requires also `ipycanvas` package and may not work in all environments (particularly Kaggle at the time of publication: september 2025). It works well in local Jupyter environments and Google Colab.\n",
    "\n",
    "This section provides an interactive drawing interface where you can:\n",
    "- Draw custom shapes directly on a canvas\n",
    "- Train the Soft Ellipse RBF model on your drawings\n",
    "- Observe how the algorithm adapts to hand-drawn non-convex shapes\n",
    "\n",
    "### Setup Interactive Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANVAS_AVAILABLE = True # Set to True if ipycanvas is installed and working(registered)\n",
    "# CANVAS_AVAILABLE = False # Set to True if ipycanvas is installed and working(registered)\n",
    "\n",
    "# ATTENTION: This code may not work on Kaggle as ipycanvas is not registered\n",
    "# You can execute this locally or in Google Colab\n",
    "\n",
    "# Requirements: ipywidgets and ipycanvas packages\n",
    "# Install via: !pip install ipywidgets ipycanvas\n",
    "\n",
    "# For Google Colab, also uncomment these lines:\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "\n",
    "if CANVAS_AVAILABLE:\n",
    "    from ipycanvas import Canvas\n",
    "    import ipywidgets as widgets\n",
    "\n",
    "    canvas_size = 56  # Optimized for performance\n",
    "\n",
    "    # Create drawing canvas\n",
    "    canvas = Canvas(width=canvas_size, height=canvas_size, sync_image_data=True)\n",
    "    canvas.layout.border = \"2px solid #333\"  # Add border for visibility\n",
    "\n",
    "    # Scale up visual display while keeping drawing resolution\n",
    "    visual_scale = 4\n",
    "    canvas.layout.width = f\"{canvas_size * visual_scale}px\"\n",
    "    canvas.layout.height = f\"{canvas_size * visual_scale}px\"\n",
    "\n",
    "    # Initialize with blue background (outside class)\n",
    "    canvas.fill_style = 'blue'\n",
    "    canvas.fill_rect(0, 0, canvas_size, canvas_size)\n",
    "\n",
    "    # Mouse interaction state\n",
    "    is_drawing = False\n",
    "    erase_mode = False\n",
    "    last_x, last_y = None, None\n",
    "\n",
    "    # Mouse event handlers for smooth drawing\n",
    "    @canvas.on_mouse_down\n",
    "    def on_mouse_down(x, y):\n",
    "        global is_drawing, last_x, last_y\n",
    "        is_drawing = True\n",
    "        last_x, last_y = x, y\n",
    "        \n",
    "        if erase_mode:\n",
    "            canvas.fill_style = 'blue'  # Erase with blue (outside)\n",
    "        else:\n",
    "            canvas.fill_style = 'red'   # Draw with red (inside)\n",
    "        canvas.fill_circle(x, y, 4)\n",
    "\n",
    "    @canvas.on_mouse_move\n",
    "    def on_mouse_move(x, y):\n",
    "        global is_drawing, last_x, last_y\n",
    "        if is_drawing and last_x is not None and last_y is not None:\n",
    "            if erase_mode:\n",
    "                canvas.stroke_style = 'blue'\n",
    "            else:\n",
    "                canvas.stroke_style = 'red'\n",
    "            canvas.line_width = 8\n",
    "            canvas.fill_circle(x, y, 4)\n",
    "            # Draw smooth line between points\n",
    "            canvas.begin_path()\n",
    "            canvas.move_to(last_x, last_y)\n",
    "            canvas.line_to(x, y)\n",
    "            canvas.stroke()\n",
    "            \n",
    "            last_x, last_y = x, y\n",
    "\n",
    "    @canvas.on_mouse_up\n",
    "    def on_mouse_up(x, y):\n",
    "        global is_drawing, last_x, last_y\n",
    "        is_drawing = False\n",
    "        last_x, last_y = None, None\n",
    "\n",
    "    # Control buttons\n",
    "    btn_clear = widgets.Button(description=\"Clear Canvas\")\n",
    "    btn_draw = widgets.ToggleButton(description=\"🔴 Draw (Red)\", button_style='success', value=True)\n",
    "    btn_erase = widgets.ToggleButton(description=\"🔵 Erase (Blue)\", button_style='', value=False)\n",
    "    out = widgets.Output()\n",
    "\n",
    "    # Mode switching functions\n",
    "    def set_draw_mode(change):\n",
    "        global erase_mode\n",
    "        if change['new']:\n",
    "            erase_mode = False\n",
    "            btn_draw.value = True\n",
    "            btn_erase.value = False\n",
    "            btn_draw.button_style = 'success'\n",
    "            btn_erase.button_style = ''\n",
    "\n",
    "    def set_erase_mode(change):\n",
    "        global erase_mode\n",
    "        if change['new']:\n",
    "            erase_mode = True\n",
    "            btn_draw.value = False\n",
    "            btn_erase.value = True\n",
    "            btn_draw.button_style = ''\n",
    "            btn_erase.button_style = 'info'\n",
    "\n",
    "    btn_draw.observe(set_draw_mode, names='value')\n",
    "    btn_erase.observe(set_erase_mode, names='value')\n",
    "\n",
    "    def clear_canvas(b):\n",
    "        \"\"\"Reset canvas to blue background.\"\"\"\n",
    "        canvas.clear()\n",
    "        canvas.fill_style = 'blue'\n",
    "        canvas.fill_rect(0, 0, canvas_size, canvas_size)\n",
    "\n",
    "    btn_clear.on_click(clear_canvas)\n",
    "\n",
    "    # Layout the interface\n",
    "    canvas_widget = widgets.VBox([canvas], layout=widgets.Layout(width='auto', align_items='center'))\n",
    "    mode_buttons = widgets.HBox([btn_draw, btn_erase], layout=widgets.Layout(justify_content='center'))\n",
    "    action_buttons = widgets.HBox([btn_clear], layout=widgets.Layout(justify_content='center'))\n",
    "    output_widget = widgets.VBox([out], layout=widgets.Layout(width='auto'))\n",
    "\n",
    "    # Create and display the main interface\n",
    "    main_interface = widgets.VBox([canvas_widget, mode_buttons, action_buttons, output_widget], \n",
    "                                layout=widgets.Layout(align_items='center'))\n",
    "    display(main_interface)\n",
    "else:\n",
    "    print(\"Canvas not available. Please install ipycanvas: pip install ipycanvas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canvas Data Processing & Training\n",
    "\n",
    "Functions to convert canvas drawings into training data and visualize the sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CANVAS_AVAILABLE:\n",
    "    def sample_canvas_points(n):\n",
    "        \"\"\"Convert canvas drawing to training data points.\n",
    "        \n",
    "        Processes the canvas image to extract binary classification data:\n",
    "        - Red pixels → inside class (label=1)\n",
    "        - Blue pixels → outside class (label=0)\n",
    "        \n",
    "        Args:\n",
    "            n: Number of data points to sample\n",
    "            \n",
    "        Returns:\n",
    "            X_data: Normalized coordinates (n, 2)\n",
    "            y_data: Binary labels (n,)\n",
    "        \"\"\"\n",
    "        img = canvas.get_image_data(0, 0, canvas_size, canvas_size)\n",
    "        \n",
    "        # Convert RGB to binary classification\n",
    "        red_channel = img[:, :, 0]\n",
    "        blue_channel = img[:, :, 2]\n",
    "        \n",
    "        # Red pixels = inside (1), Blue pixels = outside (0)\n",
    "        binary = ((red_channel > 127) & (blue_channel < 127)).astype(np.uint8)\n",
    "\n",
    "        # Generate coordinate grid\n",
    "        coords = np.array(np.meshgrid(np.arange(canvas_size), np.arange(canvas_size))).reshape(2, -1).T\n",
    "        pixel_values = binary.flatten()\n",
    "        \n",
    "        # Handle sampling with/without replacement\n",
    "        total_pixels = canvas_size * canvas_size\n",
    "        use_replacement = n > total_pixels\n",
    "        \n",
    "        if use_replacement:\n",
    "            print(f\"Note: Requested {n} samples from {total_pixels} pixels - using replacement sampling\")\n",
    "        \n",
    "        # Random sampling\n",
    "        idx = np.random.choice(len(coords), n, replace=use_replacement)\n",
    "        \n",
    "        # Extract and normalize coordinates\n",
    "        x_flat = coords[idx, 0]\n",
    "        y_flat = coords[idx, 1]\n",
    "        \n",
    "        # Normalize to [0, 1] range\n",
    "        x_normalized = x_flat / (canvas_size - 1)\n",
    "        y_normalized = (canvas_size - 1 - y_flat) / (canvas_size - 1)  # Flip Y axis\n",
    "        \n",
    "        # Combine coordinates and add adaptive noise\n",
    "        x_data = np.column_stack([x_normalized, y_normalized])\n",
    "        \n",
    "        # Adaptive noise: decreases with sample density\n",
    "        base_noise = 0.025  \n",
    "        noise_factor = min(1.0, 1000 / n)\n",
    "        noise_amount = base_noise * noise_factor\n",
    "        \n",
    "        x_data = x_data + np.random.uniform(-noise_amount, noise_amount, x_data.shape)\n",
    "        x_data = x_data.clip(1e-6, 1 - 1e-6)  # Avoid exact boundary values\n",
    "        \n",
    "        y_data = pixel_values[idx]\n",
    "        return x_data, y_data\n",
    "\n",
    "    def visualize_canvas_sampling(n=1000):\n",
    "        \"\"\"Visualize the canvas sampling process and data distribution.\n",
    "        \n",
    "        Shows side-by-side comparison of:\n",
    "        1. Original canvas drawing (processed binary image)\n",
    "        2. Sampled data points with class distribution\n",
    "        \"\"\"\n",
    "        X_sample, y_sample = sample_canvas_points(n)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Process canvas image\n",
    "        img = canvas.get_image_data(0, 0, canvas_size, canvas_size)\n",
    "        red_channel = img[:, :, 0]\n",
    "        blue_channel = img[:, :, 2]\n",
    "        \n",
    "        # Create binary classification visualization\n",
    "        binary = ((red_channel > 127) & (blue_channel < 127)).astype(np.uint8)\n",
    "        \n",
    "        # RGB visualization of processed canvas\n",
    "        canvas_rgb = np.zeros((canvas_size, canvas_size, 3), dtype=np.uint8)\n",
    "        canvas_rgb[:, :, 0] = binary * 255      # Red channel for inside\n",
    "        canvas_rgb[:, :, 2] = (1 - binary) * 255  # Blue channel for outside\n",
    "        \n",
    "        ax1.imshow(canvas_rgb, origin='upper')\n",
    "        ax1.set_title(f\"Processed Canvas (Binary {canvas_size}x{canvas_size})\\n(Red=Inside, Blue=Outside)\")\n",
    "        ax1.set_xlabel(\"X (canvas)\")\n",
    "        ax1.set_ylabel(\"Y (canvas, top=0)\")\n",
    "        \n",
    "        # Show sampled points\n",
    "        colors = ['blue' if val == 0 else 'red' for val in y_sample]\n",
    "        n_inside = np.sum(y_sample)\n",
    "        n_outside = n - n_inside\n",
    "        \n",
    "        ax2.scatter(X_sample[:, 0], X_sample[:, 1], c=colors, edgecolor=\"none\", s=8, alpha=0.8)\n",
    "        ax2.set_title(f\"Sampled Points (n={n})\\n(Red={n_inside}, Blue={n_outside})\")\n",
    "        ax2.set_xlabel(\"X (plot)\")\n",
    "        ax2.set_ylabel(\"Y (plot, bottom=0)\")\n",
    "        ax2.set_xlim(0, 1)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.set_aspect('equal')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Sampled {n} points: {n_inside} inside (red), {n_outside} outside (blue)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CANVAS_AVAILABLE:\n",
    "    # Visualize the sampling process\n",
    "    visualize_canvas_sampling(n=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model on Canvas Drawing\n",
    "\n",
    "Sample data from your canvas drawing and train the Soft Ellipse RBF model on your custom shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CANVAS_AVAILABLE:\n",
    "    # Visualize the sampling process\n",
    "    # visualize_canvas_sampling(n=3000)\n",
    "\n",
    "    # Train model on canvas data\n",
    "    RndSeed.set_seed(197)  # Fixed seed for reproducibility\n",
    "\n",
    "    # Generate samples from your canvas drawing\n",
    "    X_canvas, y_canvas = sample_canvas_points(n=3000)\n",
    "\n",
    "    model_canvas, iou_canvas, _, _, _, _ = build_and_train_model(\n",
    "        X_canvas, y_canvas, \n",
    "        b_validation=True, \n",
    "        plot_frame_interval=5, \n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "This notebook showed how **Soft Elliptical RBFs** can be used for non-convex shape classification.  \n",
    "Key takeaways:  \n",
    "\n",
    "- **Targeted unit placement** with DBSCAN helps the model focus on errors  \n",
    "- **Incremental learning** builds up complexity step by step  \n",
    "- **Compact and efficient** thanks to pruning and medoid selection  \n",
    "- **Versatile** enough to handle donuts, stars, and even hand-drawn shapes  \n",
    "\n",
    "Overall, it’s a practical way to combine neural networks with clear geometric meaning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 🔗 See also\n",
    "\n",
    "👈 *Previous*: [Convex Polygon Shape Classification with Custom Activations](<https://github.com/Dani-Luk/kaggle-wrapping-convex-shape-classifier/tree/main>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
